{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Data from the Web\n",
    "=====================================\n",
    "\n",
    "For today we are going to look at creating a few scripts that we can use to scrape data from the web in a couple\n",
    "of different fields.  \n",
    "\n",
    "1. Medical Data\n",
    "2. Sports Data\n",
    "3. General News Articles\n",
    "\n",
    "We are going to use `requests` and `beautifulsoup4` to accomplish this task.  The first step is to determine the sources\n",
    "of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start going through the different sources, the first thing that we want to create is a helper function\n",
    "that we can use to easily return the `status_code` and the `content`.  We also want it to retry at least 3 times\n",
    "in the case of a failed url (likely introducing some sort of wait between requests).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_get(url, retry_count=3):\n",
    "    attempt_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            attempt_count += 1\n",
    "            result = requests.get(url)\n",
    "            code = result.status_code\n",
    "            content = result.content\n",
    "            \n",
    "            if code >= 200 and code < 300:\n",
    "                return code, content\n",
    "\n",
    "            if attempt_count > retry_count:\n",
    "                return code, content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Url {url} failed {attempt_count} times - last error {e}')\n",
    "            if attempt_count > retry_count:\n",
    "                return -1, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Sources\n",
    "\n",
    "Today we are groing to scrap another medical news site, in this case we are going to grab data \n",
    "from [News Medical](https://www.news-medical.net/medical/news).  This page contains a number of\n",
    "articles that date back as far as __2009__.  \n",
    "\n",
    "First lets look at the page that lists the articles (in a paged fashion).  That link is\n",
    "[News Medical Page 1 - https://www.news-medical.net/medical/news?page=1](https://www.news-medical.net/medical/news?page=1).  \n",
    "\n",
    "If we look at the structure of the list of articles on the page (there are 20 per page) we see something like the below. \n",
    "\n",
    "        <div class=\"posts\">\n",
    "            <div class=\"row\">\n",
    "                <div>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <h3>\n",
    "                        <a href=\"--article-link--\">--Article title--</a>\n",
    "                    </h3>\n",
    "                    <p class=\"item-desc\">\n",
    "                        --Article Description--\n",
    "                    </p>\n",
    "                </div>\n",
    "            </div>\n",
    "            ...\n",
    "        </div>\n",
    "\n",
    "So looking at this format, I believe we are going to grab all the `item-desc` elements and the `h3` siblings\n",
    "from the `posts` parent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_medical_news_page(page_number=None):\n",
    "    base_url = 'https://www.news-medical.net'\n",
    "    news_url = f'{base_url}/medical/news'\n",
    "    if page_number is not None:\n",
    "        page_url = f'{news_url}?page={page_number}'\n",
    "    else:\n",
    "        page_url = news_url\n",
    "        \n",
    "    try:\n",
    "        status_code, content = retry_get(page_url)\n",
    "\n",
    "        if status_code < 200 or status_code >= 300:\n",
    "            return status_code, []\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html5lib')\n",
    "\n",
    "        posts = soup.find('div', class_='posts')\n",
    "        articles = [p.parent for p in posts.find_all('p', class_='item-desc')]\n",
    "        \n",
    "        return_articles = []\n",
    "        for article in articles:\n",
    "            a_tag = article.find('a')\n",
    "            article_link = f'{base_url}{a_tag[\"href\"]}'\n",
    "            article_desc = article.find('p').get_text().strip()\n",
    "            article_header = a_tag.get_text()\n",
    "            return_articles.append({\n",
    "                'link': article_link,\n",
    "                'desc': article_desc,\n",
    "                'title': article_header\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Failed to get details from {archive_url} - {e}')\n",
    "        return -1, []\n",
    "    \n",
    "    return status_code, return_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have a way to grab all the article links on a listing page.  Lets look at an article to determine\n",
    "what the structure is there and how to extract data from the page.  \n",
    "\n",
    "To start here is the structure we are looking at.  \n",
    "\n",
    "    <div class='item-body content-item-body'>\n",
    "        <h1 itemprop='headline'>--Article title--</h1>\n",
    "        ...\n",
    "        <div class='content'>\n",
    "            <div itemprop='articleBody'>\n",
    "                <div class='article-meta'>\n",
    "                     <span class='article-meta-contents'>\n",
    "                         <span class='article-meta-date'>Month Day, Year</span>\n",
    "                     </span>\n",
    "                </div>\n",
    "                ...\n",
    "                <p>--Article Paragraph--</p>\n",
    "                <p>--Article Paragraph--</p>\n",
    "                ...\n",
    "                <p>--Article Paragraph--</p>\n",
    "                <p>--Article Paragraph--</p>\n",
    "                ...    \n",
    "            </div>        \n",
    "        </div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news_medical_article(url):\n",
    "    try:\n",
    "        status_code, content = retry_get(url)\n",
    "        \n",
    "        if status_code < 200 or status_code >= 300:\n",
    "            return status_code, str(content)\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html5lib')\n",
    "        article = soup.find('div', class_='content-item-body')\n",
    "        article_content = article.find('div', itemprop='articleBody')\n",
    "        \n",
    "        article_headline = article.find('h1', itemprop='headline').get_text().strip()\n",
    "        article_date = article_content.find('span', class_='article-meta-date').get_text().strip()\n",
    "        article_paragraphs = [p.get_text().strip() for p in article_content.find_all('p')]\n",
    "        \n",
    "        return status_code, {\n",
    "            'headline': article_headline,\n",
    "            'date': article_date,\n",
    "            'text': '\\n--\\n'.join(article_paragraphs),\n",
    "            'paragraphs': article_paragraphs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read article {url} - {e}')\n",
    "        return -1, None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have the two helper methods needed to read in a list of articles and get the content of said articles.  Lets\n",
    "now create a helper function that we can use to retrieve all the articles for a list of `page_numbers`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_news_medical(page_numbers, save_filename):\n",
    "    results = {}\n",
    "    for page_number in page_numbers:\n",
    "        print(f'Reading page_number {page_number} of {page_numbers}')\n",
    "        code, articles = get_articles_from_medical_news_page(page_number)\n",
    "        if code < 200 or code >= 300:\n",
    "            results[page_number] = (code, articles)\n",
    "            continue\n",
    "            \n",
    "        for article in articles:\n",
    "            code, result = read_news_medical_article(article['link'])            \n",
    "            article['status_code'] = code\n",
    "            article['result'] = result\n",
    "\n",
    "        results[page_number] = articles\n",
    "    \n",
    "    with open(save_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright we have the method, lets test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading page_number 1 of [1]\n"
     ]
    }
   ],
   "source": [
    "#read_from_news_medical([1], '/tmp/news_medical.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright we have a working page, so now lets create a crawler that will go through groups and write them out to file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       " [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
       " [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
       " [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       " [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n",
       " [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],\n",
       " [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
       " [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\n",
       " [91, 92, 93, 94, 95, 96, 97, 98, 99, 100],\n",
       " [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
       " [111, 112, 113, 114, 115, 116, 117, 118, 119, 120],\n",
       " [121, 122, 123, 124, 125, 126, 127, 128, 129, 130],\n",
       " [131, 132, 133, 134, 135, 136, 137, 138, 139, 140],\n",
       " [141, 142, 143, 144, 145, 146, 147, 148, 149, 150],\n",
       " [151, 152, 153, 154, 155, 156, 157, 158, 159, 160],\n",
       " [161, 162, 163, 164, 165, 166, 167, 168, 169, 170],\n",
       " [171, 172, 173, 174, 175, 176, 177, 178, 179, 180],\n",
       " [181, 182, 183, 184, 185, 186, 187, 188, 189, 190],\n",
       " [191, 192, 193, 194, 195, 196, 197, 198, 199, 200]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "page_size = 10\n",
    "max_groups = 20\n",
    "page_groups = [list(range(1 + i * page_size, (1 + page_size) + (i * page_size))) for i in range(max_groups)]\n",
    "display(page_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_nums in page_groups:\n",
    "    print(f'Handling page nums {page_nums}')\n",
    "    file_name = f'news-medical-{page_nums[0]}-{page_nums[-1]}.json'\n",
    "    read_from_news_medical(page_nums, os.path.join('..', 'raw_data', 'docs', 'medical', file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
