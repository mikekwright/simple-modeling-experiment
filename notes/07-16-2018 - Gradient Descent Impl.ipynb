{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "-------------------------------\n",
    "\n",
    "Today I am going to implement the function for gradient descent function as defined by Andrew Ng\n",
    "in his Machine Learning Coursera course.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that I am going to do is to define the gradient descent function for \n",
    "the number of features ($n$), with a training set of size ($m$)\n",
    "\n",
    "Cost Function:\n",
    "    $$\n",
    "     J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "    $$\n",
    "    \n",
    "Gradient Descent (_Repeat until Convergence_):\n",
    " \n",
    "  $$\n",
    "  \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})* x_0^{(i)}    \n",
    "  $$\n",
    "  $$\n",
    "  \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})* x_1^{(i)}\n",
    "  $$\n",
    "  $$\n",
    "  ...\n",
    "  $$\n",
    "  $$\n",
    "  \\theta_n := \\theta_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})* x_n^{(i)}\n",
    "  $$\n",
    "  \n",
    "> Note: There are two things to call out, the first being that $x_0$ is always going to be $1$ as we\n",
    "are using this weight as our __bias__.  Also the function we are using above is a partial derivative\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial\\theta_0} = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})\n",
    "  $$\n",
    "\n",
    "So now that we have this math in mind, lets go ahead an code this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights, X, y, learning_rate=0.001, convergence=0.001, loops=None):\n",
    "    assert len(X) == len(y)\n",
    "    assert len(weights) == len(X[0])+1  # The extra one is because we are treating x_0 as a 1 and the weight as the bias\n",
    "    \n",
    "    loops = loops or 10\n",
    "    \n",
    "    X_with_0 = [[1] + X_i for X_i in X]\n",
    "    \n",
    "    def h_func(X_i, weights):\n",
    "        return sum([x * w for x,w in zip(X_i, weights)])\n",
    "        \n",
    "    for i in range(loops):        \n",
    "        sample_size = len(X)\n",
    "        updated_weights = list(weights)\n",
    "        for j in range(len(weights)):\n",
    "            updated_weights[j] = weights[j] - learning_rate * (1 / sample_size) * \\\n",
    "               sum([((h_func(X_i, weights) - y_i) * X_i[j]) for X_i, y_i in zip(X_with_0, y)])\n",
    "            \n",
    "        weights = updated_weights\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, lets test our function out.  We have the following function that we are trying to predict.  \n",
    "\n",
    "$$\n",
    "y = 10 + 3x\n",
    "$$\n",
    "\n",
    "We are going to make this an easy example by having the following training examples (no noise).  \n",
    "\n",
    "$$\n",
    "m = 4\n",
    "$$\n",
    "$$\n",
    "X = [[7], [2], [10], [5]]\n",
    "$$\n",
    "$$\n",
    "y = [31, 16, 40, 25]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93697586, 0.85850861])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[7], [2], [10], [5]]\n",
    "y = [31, 16, 40, 25]\n",
    "\n",
    "weights = np.random.rand(2)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we have our data and our weights (randomly selected).  Lets see if we can converge on the solution after 100 loops.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5406642823006271, 4.112785570434361]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we are not there... we can try to see if adjusting the learning rate helps or hinders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8596959094733556, 3.966813161875216]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is only a little better... so lets try it with a few more loops.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.682296034016924, 3.1784200674951433]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So bumping us up from 100 to 1000 had a huge impact... we are almost there... lets bump it up to 10000 now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.999999939628895, 3.000000008174383]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=10000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is pretty good.  For all intents and purposes we were able to correct pick the weights that\n",
    "completed our function.  \n",
    "\n",
    "However this was a very simple function... for our next use case lets create a function that has more variables.  \n",
    "\n",
    "$$\n",
    "y = 1.2 + 2.5(a) + 1.3(b) - 0.5(c)\n",
    "$$\n",
    "\n",
    "And, lets create a training set for this data.  \n",
    "\n",
    "$$\n",
    "m = 6\n",
    "$$\n",
    "$$\n",
    "X = [[2, 4, 3], [1, 2, 1], [1, 4, 4], [2, 3, 1], [9, 3, 10], [8, 11, 6]]\n",
    "$$\n",
    "$$\n",
    "y = [9.9, 5.800000000000001, 6.9, 9.600000000000001, 22.6, 32.5]\n",
    "$$\n",
    "\n",
    "__Note__: The easiest ways to get Y in this case is just to run the below code.  \n",
    "        \n",
    "        y = [(1.2 + (2.5 *i[0]) + (1.3 * i[1]) - (0.5 * i[2])) for i in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22927249, 0.26481764, 0.70905633, 0.12221558])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[2, 4, 3], [1, 2, 1], [1, 4, 4], [2, 3, 1], [9, 3, 10], [8, 11, 6]]\n",
    "y = [9.9, 5.800000000000001, 6.9, 9.600000000000001, 22.6, 32.5]\n",
    "\n",
    "weights = np.random.rand(4)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, lets try this with a `learning_rate` of `0.01` and with 1000 loops to start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9990714368777828, 2.46424228663451, 1.3273869935038272, -0.4574399278531272]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to bad for the first 1000, lets bump this up to 10000 to see if it gets it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1999999896304487, 2.499999998154821, 1.300000001413358, -0.4999999978037285]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=10000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, another success!  So the final example is going to a be a simpiler function, however we\n",
    "are going to set the bias as a random value.  So for this one, every values of $y$ for a given $X$\n",
    "will be created using the below function.  \n",
    "\n",
    "$$\n",
    "y = random + 1.5(a) + 0.24(b)\n",
    "$$\n",
    "\n",
    "We will use the following training set.  \n",
    "\n",
    "$$\n",
    "m = 6\n",
    "$$\n",
    "$$\n",
    "X = [[1, 2], [4, 5], [3, 8], [9, 12], [20, 5], [-2, 1]]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.6541430502578685,\n",
       " 11.475176769715041,\n",
       " 13.645518577912771,\n",
       " 26.3388038205697,\n",
       " 35.40210005589027,\n",
       " -1.4848737940545111]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.00182814, 0.21736791, 0.91668627])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 2], [4, 5], [3, 8], [9, 12], [20, 5], [-2, 1]]\n",
    "y = [np.random.rand() + (1.5 * i[0]) + (0.24 + i[1]) for i in X]\n",
    "display(y)\n",
    "\n",
    "weights = np.random.rand(3)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright... this time we are just going to do a full gradient descent over 10000 loops since that\n",
    "seems to be the best outcome so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6973566170296959, 1.4753706689947659, 1.0366289504029107]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=10000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... so this is interesting... the $a$ weight seems about right, but the $b$ weight should be $0.24$ and it is $1.03..$ \n",
    "\n",
    "Lets see if changing the loops would adjust the count or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6973566170297408, 1.4753706689947652, 1.0366289504029056]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(weights, X, y, loops=100000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So introducing the notion of random is causing issues in the outcome... now it is important to note that our training set\n",
    "was not that large, lets see if it changes if our training set is a bit bigger.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70038892, 0.04038746, 0.42876696])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [list(np.random.rand(2)) for _ in range(100)]\n",
    "y = [np.random.rand() + (1.5 * i[0]) + (0.24 + i[1]) for i in X]\n",
    "\n",
    "X_test = [list(np.random.rand(2)) for _ in range(10)]\n",
    "y_test = [np.random.rand() + (1.5 * i[0]) + (0.24 + i[1]) for i in X_test]\n",
    "\n",
    "weights = np.random.rand(3)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7988220442282105, 1.3890722281566, 0.9841294134371793]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = gradient_descent(weights, X, y, loops=10000, learning_rate=0.01)\n",
    "trained_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check out or error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05636550442509569,\n",
       " 0.014352208553011225,\n",
       " 0.17928726424453334,\n",
       " 0.2544133629260583,\n",
       " 0.10199611080334448,\n",
       " 0.4954440103818616,\n",
       " 0.24044108226391137,\n",
       " 0.42843969631334833,\n",
       " 0.27286159284824674,\n",
       " 0.2599763325562625]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2303577165315674"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = [(trained_weights[0] + sum([t * i for t, i in zip(trained_weights[1:], x)])) for x in X_test]\n",
    "error = [np.abs(y_t - y_p) for y_t, y_p in zip(y_test, y_predict)]\n",
    "display(error)\n",
    "sum(error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok for this next example I just want to try the solution on a polynomial function. \n",
    "\n",
    "$$\n",
    "y = 3 + 2.5a + 4b + 1.8a^2 + 0b^2 + 1.1(a*b)\n",
    "$$\n",
    "\n",
    "We will also have a sample set of `10` items that are randomly generated, with the corresponding targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79076562, 0.14551373, 0.54934079, 0.01278833, 0.04125727,\n",
       "       0.77419747])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = [list(np.random.rand(2)) for _ in range(110)]\n",
    "y_all = [(3 + (2.5 * i[0]) + (4 * i[1]) + (1.8 * np.exp(i[0])) + (1.1 * i[0] * i[1])) for i in X_all]\n",
    "\n",
    "X = [[i[0], i[1], np.exp(i[0]), np.exp(i[1]), i[0] * i[1]] for i in X_all[:-10]]\n",
    "y = y_all[:-10]\n",
    "\n",
    "X_test = [[i[0], i[1], np.exp(i[0]), np.exp(i[1]), i[0] * i[1]] for i in X_all[-10:]]\n",
    "y_test = y_all[-10:]\n",
    "\n",
    "weights = np.random.rand(6)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9747519479012845,\n",
       " 2.48389197741206,\n",
       " 3.967085419441315,\n",
       " 1.809588801517826,\n",
       " 0.019184573568546547,\n",
       " 1.0999038302884239]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = gradient_descent(weights, X, y, loops=30000, learning_rate=0.2)\n",
    "trained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002259869130130099,\n",
       " 0.0010997269584400726,\n",
       " 0.000293110361134552,\n",
       " 0.001597832841696345,\n",
       " 0.001914420037712361,\n",
       " 0.0016914197111939089,\n",
       " 0.0026342602528544035,\n",
       " 0.002129017828453428,\n",
       " 0.0011504967980453529,\n",
       " 0.0007488690675998555]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0015519022987260378"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = [(trained_weights[0] + sum([t * i for t, i in zip(trained_weights[1:], x)])) for x in X_test]\n",
    "error = [np.abs(y_t - y_p) for y_t, y_p in zip(y_test, y_predict)]\n",
    "display(error)\n",
    "sum(error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the solution seems to work with polynomials as well, however I did have to do some hyperparameter tuning to come\n",
    "up with a relatively good solution.  These are some combinations that I used. \n",
    "\n",
    "- `learning_rate`: 0.01, `loops`: 10000\n",
    "- `learning_rate`: 0.1, `loops`: 10000\n",
    "- `learning_rate`: 0.1, `loops`: 100000\n",
    "- `learning_rate`: 0.2, `loops`: 10000\n",
    "- `learning_rate`: 0.2, `loops`: 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving using Normal Equation\n",
    "\n",
    "One thing that we didn't call out, is that we are basically trying to find the location where the derivative for $\\theta$\n",
    "is equal to 0.  For a single feature this is a pretty simple, however for multiple features what we are trying to do is find\n",
    "the partial derivate for each $\\theta_j$ in our $m$ features.  \n",
    "\n",
    "Luckily this already has a solution for us, using matrices. The function is defined below.  \n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Now it is important to understand the structure of both $X$ and $y$.  Namely we will need to make sure add hardcoded `1` for\n",
    "use in calculating the `bias` value.  \n",
    "\n",
    "$$\n",
    " X = \\begin{bmatrix}\n",
    "    1 & 3 & 2 & 5 \\\\\n",
    "    1 & 4 & 5 & 1 \\\\\n",
    "    ... \\\\\n",
    "    1 & 2 & 8 & 3 \\\\   \n",
    "  \\end{bmatrix}\n",
    "  Y = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    22 \\\\\n",
    "    ... \\\\\n",
    "    13\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So lets go ahead and create a method for the normal equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    X = np.array(X)\n",
    "    bias_column = np.ones((len(X), 1))\n",
    "    X = np.append(bias_column, X, axis=1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    y = y.reshape((len(X), 1))\n",
    "    \n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    #return np.invert(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.00000000e+00],\n",
       "       [ 2.50000000e+00],\n",
       "       [ 4.00000000e+00],\n",
       "       [ 1.80000000e+00],\n",
       "       [-1.09956488e-12],\n",
       "       [ 1.10000000e+00]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = normal_equation(X, y)\n",
    "trained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.74811293e-13]),\n",
       " array([3.19744231e-14]),\n",
       " array([8.70414851e-13]),\n",
       " array([2.06057393e-13]),\n",
       " array([6.76791956e-13]),\n",
       " array([9.14823772e-14]),\n",
       " array([1.47615253e-12]),\n",
       " array([9.05941988e-13]),\n",
       " array([2.71782596e-13]),\n",
       " array([2.09610107e-13])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5.11501952e-13])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = [(trained_weights[0] + sum([t * i for t, i in zip(trained_weights[1:], x)])) for x in X_test]\n",
    "error = [np.abs(y_t - y_p) for y_t, y_p in zip(y_test, y_predict)]\n",
    "display(error)\n",
    "sum(error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is running much better now, it is fast, requires no iterations, however it will likley not be a good result if the\n",
    "number of features or number of samples are to large.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the Solution\n",
    "\n",
    "At this point we have a naive, python solution that manually iterates through each value... and it is slow.\n",
    "For this next step lets go ahead and `vectorize` the solution using numpy.  \n",
    "\n",
    "To make things simplier, lets look at the algorithm again.  Gradient descent is defined below (for a single value of $\\theta$).  \n",
    "\n",
    "$$\n",
    "  \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})* x_0^{(i)}    \n",
    "$$\n",
    "\n",
    "Looking at this, we want to vectorize the solution and I am going to break up the above into a few pieces that can be vectorized.\n",
    "\n",
    "The first piece is $(h_\\theta(x^{(i)}) - y^{(i)})$ which we will call $H$ so now our algorithm looks like this.  \n",
    "\n",
    "$$\n",
    "  \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}H* x_0^{(i)}    \n",
    "$$\n",
    "\n",
    "The next thing is to realize that we are going to use all the values of of $x$ instead of just $x_0$ we will call this value $x$\n",
    "Because we are removing the subscript for $x$ we are also goign to do so from $\\theta$ so now our solution looks like this.  \n",
    "\n",
    "$$\n",
    "  \\theta := \\theta - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}H* x^{(i)}    \n",
    "$$\n",
    "\n",
    "The final piece is the summation and the division of $\\frac{1}{m}$ so we can replace that entire section \n",
    "$\\frac{1}{m}\\sum_{i=1}^{m}H* x^{(i)}$ body with a single value of $R$ so our final equation looks like this.  \n",
    "\n",
    "$$\n",
    "  \\theta := \\theta - \\alpha{R}\n",
    "$$\n",
    "\n",
    "So lets go ahead and vectorize our below solution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights, X, y, learning_rate=0.001, convergence=0.001, loops=None):\n",
    "    assert len(X) == len(y)\n",
    "    assert len(weights) == len(X[0])+1  # The extra one is because we are treating x_0 as a 1 and the weight as the bias\n",
    "    \n",
    "    loops = loops or 10\n",
    "    \n",
    "    weight_len = len(weights)\n",
    "    weights = weights.reshape((weight_len, 1))\n",
    "    m = len(X)\n",
    "    X = np.concatenate([np.ones((m,1)), np.array(X)], 1)\n",
    "    y = np.array(y).reshape((m,1))\n",
    "    \n",
    "    for i in range(loops):\n",
    "        H = (X @ weights) - y\n",
    "        H = H * X\n",
    "        R = np.sum(H, axis=0) / m\n",
    "        weights = weights - (learning_rate * R.reshape((weight_len, 1)))\n",
    "            \n",
    "    return weights.reshape((weight_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59675837, 0.32754248, 0.13267929, 0.13936937, 0.2831722 ,\n",
       "       0.76037036])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = [list(np.random.rand(2)) for _ in range(110)]\n",
    "y_all = [(3 + (2.5 * i[0]) + (4 * i[1]) + (1.8 * np.exp(i[0])) + (1.1 * i[0] * i[1])) for i in X_all]\n",
    "\n",
    "X = [[i[0], i[1], np.exp(i[0]), np.exp(i[1]), i[0] * i[1]] for i in X_all[:-10]]\n",
    "y = y_all[:-10]\n",
    "\n",
    "X_test = [[i[0], i[1], np.exp(i[0]), np.exp(i[1]), i[0] * i[1]] for i in X_all[-10:]]\n",
    "y_test = y_all[-10:]\n",
    "\n",
    "weights = np.random.rand(6)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.31953814, 1.2395394 , 0.67826498, 2.31101111, 1.779575  ,\n",
       "       1.2840817 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = gradient_descent(weights, X, y, loops=10000)\n",
    "trained_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weights from a vectorized solution, lets see what our error looks like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4662471887637345,\n",
       " 0.06909434327168995,\n",
       " 0.37320147154070327,\n",
       " 0.13137964775737032,\n",
       " 0.2013382151436831,\n",
       " 0.14950287614054503,\n",
       " 0.057902822268445675,\n",
       " 0.12831205868814344,\n",
       " 0.07050953313523678,\n",
       " 0.13597349102606593]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.1783461647735618"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = [(trained_weights[0] + sum([t * i for t, i in zip(trained_weights[1:], x)])) for x in X_test]\n",
    "error = [np.abs(y_t - y_p) for y_t, y_p in zip(y_test, y_predict)]\n",
    "display(error)\n",
    "sum(error)/len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
